---
output:
  word_document: default
  html_document: default
---
#### Loading the data into R

The data was tidied up in excel, by creating a new variable for the time (1970 to 2014) which was titled "Year"; and also, a variable (Country) was created to differentiate between the countries as all the 14 sets of the 2 categories (as the residual was removed) were joined together as a single variable to enable proper access and analysis. Furthermore, the headers of the variables was changed into more self-explaining names (e.g The variable y was changed to log_GDP which tells us it is a variable that measures the log of GDP) Afterwards, the cleaned up data was saved in CSV (Comma Separated Values) format before it was imported into R

```{r}
cleanData = read.csv("panelClean.csv", header = T)
head(cleanData)
tail(cleanData)

summary(cleanData)
```


#### Specifying the data as a Panel Data to R

The data has been read into R just like any other dataset, but there is need to specify the data as a panel data to R. The R Package for which the whole of this analysis will be run is loaded as well as the data specification for R.

```{r}
# Loading in the library (i.e. the package to be used for the analysis)
library(PSTR)


```


```{r}
library(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')



dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)
dtest <- xgb.DMatrix(agaricus.test$data, label = agaricus.test$label)
watchlist <- list(train = dtrain, eval = dtest)

## A simple xgb.train example:
param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2,
              objective = "binary:logistic", eval_metric = "auc")
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist)


## An xgb.train example where custom objective and evaluation metric are used:
logregobj <- function(preds, dtrain) {
   labels <- getinfo(dtrain, "label")
   preds <- 1/(1 + exp(-preds))
   grad <- preds - labels
   hess <- preds * (1 - preds)
   return(list(grad = grad, hess = hess))
}
evalerror <- function(preds, dtrain) {
  labels <- getinfo(dtrain, "label")
  err <- as.numeric(sum(labels != (preds > 0)))/length(labels)
  return(list(metric = "error", value = err))
}

# These functions could be used by passing them either:
#  as 'objective' and 'eval_metric' parameters in the params list:
param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2,
              objective = logregobj, eval_metric = evalerror)
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist)

#  or through the ... arguments:
param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2)
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
                 objective = logregobj, eval_metric = evalerror)

#  or as dedicated 'obj' and 'feval' parameters of xgb.train:
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
                 obj = logregobj, feval = evalerror)


## An xgb.train example of using variable learning rates at each iteration:
param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2,
              objective = "binary:logistic", eval_metric = "auc")
my_etas <- list(eta = c(0.5, 0.1))
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
                 callbacks = list(cb.reset.parameters(my_etas)))

## Early stopping:
bst <- xgb.train(param, dtrain, nrounds = 25, watchlist,
                 early_stopping_rounds = 3)

## An 'xgboost' interface example:
bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,
               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,
               objective = "binary:logistic")
pred <- predict(bst, agaricus.test$data)


```








































































